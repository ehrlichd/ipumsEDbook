--- 
title: "Data Science for Social Scientists: An applied course using IPUMS data"     
author: | 
  | ![](ipums_banner.png)
  | <br>  
  | <br>  
  | __Developed by:__  
  |    Daniel E. Ehrlich, [IPUMS, University of Minnesota](https://international.ipums.org/international/)  
  |    Anna Tremblay, [Dept of Soc, Anth, & CJ, Clemson University](https://www.clemson.edu/cbshs/departments/sacj/degrees/anthropology.html)  
  |    Current Version compiled: `r Sys.Date()`
  | ![](ipums_i_logo.jpg){width=40%} ![](clemson_logo.png){width=40%}
  | <br> <br>
site: bookdown::bookdown_site
output: 
  bookdown::gitbook:
    split_by: section
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
github-repo: ehrlichd/ipumsEDbook
description: "This is an open source book. The output format for this example is bookdown::gitbook."
---



# Preface {-}


An applied methods class for social scientists that uses real-world IPUMS data. This course is:
<h5>
| Open-source and customizable - 
|    All materials available on [Github](https://github.com/ehrlichd/ipumsEDbook) 
|
| Made with open-source tools -
|   [R](https://cran.r-project.org/), [RStudio](https://www.rstudio.com/products/rstudio/), [bookdown](https://bookdown.org/)  
|
| Driven by ^(nearly)^ open-source data -
|   Harmonized across time and space: [IPUMS](https://ipums.org)
|

</h5>



### Why make this course {-}


In a world where information and data are increasingly accessible, it is of utmost importance for individuals to understand data science and the interpretation of data. We believe that education should be easily accessible and teaching resources should be freely available to aid in this endeavor. While we (DEE) may be slightly biased, we think IPUMS is a fantastic resource for both **Education** and **Research**. Real-world example datasets provide the bulk of the content for this course, providing an applied context we hope students (and instructors) will find engaging. We also know many instructors may be teaching across multiple disciplines, in large departments, or be the only “data person” at their institution. We think IPUMS data is useful to virtually any social science field. We provide some example lessons, and encourage instructors to develop their own, using our `lesson_template.Rmd` to tailor this course to their subject or interest.



### What is IPUMS {-}

IPUMS started as a project to digitize the historical records of the US census.
It has expanded to include [9 data collections](https://www.ipums.org/), 
which are united in their methods and principles of making social science
research easier. IPUMS data consists of individual-level census and survey data
from more than 100 countries around the world. Notably: 


  * IPUMS **harmonizes** these data - ensuring consistently coded values across time and space.
  * IPUMS provides harmonized **GIS Shapefiles** for most census and survey data.
  * IPUMS provides extensive **metadata**, including:
    + Original questionnaire text.
    + Universe definition and comparability statements.
    + Alerts about notable changes in variable definition, 
    universe, or coding schema 
    


IPUMS data is free to use for education and research purposes. Researchers need
only to register with an email address and brief project description. Nothing
too formal - we’re just trying to understand what kinds of questions
researchers are interested in. For educators, we have additional resources
to facilitate set up of classroom accounts - making it easy to get your
students registered and share IPUMS data with them.



### What is R {-}

R is a programming language. Learning to use R is learning how to code, which teaches logic, and programmatic thinking.
Since R is a *statistical* programming language, it has many built-in features to facilitate a range of mathematical calculations. 
Since R is open-source, it is customizable and expandable! Base R refers to the core set of functions needed to run R. It's the bare-minimum to use R, available from CRAN. Base R can be expanded by downloading add-on R packages, either from CRAN, from a github.page, or by making your own!

While you *can* interact with R by itself, pretty much everyone agrees the experience can be better. We reccomend using Rstudio, which provides a GUI and many additional handy features that make coding in R fun!

In addition to a GUI, Rstudio is an Integrated Development Environment (IDE), which allowes a user to both write and run code, but also develop R packages ( or textbooks).

**NOTE:** 
 * Rstudio as an organization is now known/rebranding as posit
 * Rstudio as a program is now known/will be produced as quarto
   * This is because the quarto IDE supports python, javascript, etc in addition to R


This book, and all analyses, are done using R. 
**ADD IN CONTENT FROM CURRENT 1.2 - WHAT IS R**



### Getting Started {-}


In order to use this textbook, you will need to:  

  * download and install [RStudio](https://posit.co/download/rstudio-desktop/) 
    + This link also contains instructions and links to download [R from CRAN](https://cran.rstudio.com/) 
    + Be sure you download the appropriate file for your Mac or PC
    
  * Register for an with [IPUMS](https://ipums.org) account. We provide **limited example data**, but in order take full advantage of these exercises: 
    + [IPUMS registration for individuals](https://international.ipums.org/international-action/menu)
    + [IPUMS registration for instructors](https://international.ipums.org/international/classroom_accounts.shtml)


## Course Description {-}

This course is broken down into 3, 5-week units. Unit 1 focuses on familiarizing yourself with R and the IPUMS dataset. In Unit 2, each week will showcase a method/analysis using preselected variables. In class, students will walk through a given problem set and produce a lab report by the end of class. In Unit 3, students will work towards answering a research question that they pose, creating a research paper with literature review, data analysis, conclusion, and data outputs.


### Course Aims {-}

Provide students with relevant, hands on, methodological training in data literacy and visualization. 



### Learning Outcomes {-}

After this course, students will be able to:

  * Understand the depth of the IPUMS database and the variables it has to  
offer
  * Compose R code to analyze the IPUMS data
  * Produce visually pleasing data outputs in R
  * Synthesize the information in a written report
  * Present the analysis in a poster format for other students


### Guiding Principles {-}

* Phenomenon-based learning
  + try to start the class with a **question** or **problem**
  + *why* does the data look the way it does
  + structure class so students work towards solving the problem
* Relevant examples
  + Drawn from multiple disciplines (eg, economics, demography)
  + Can be added as modular examples/exercises







## Syllabus - General {-}

This syllabus is initially envisioned as 3 5-week sections. However, compilation and content are intended to be modular with templates for instructors to include their own specialties.

The basic structure of this course is:

**Unit 1 (Weeks 1-5):** Understanding and Testing Data

  * Students use simple datasets bundled with the course or provided by the instructor. 
  * Simplified data to illustrate trends.
      + EG: plotting continuous variable (AGE); Table of categorical variable (SEX); Crosstabs

**Unit 2 (Weeks 6-10):** Finding Data and Asking Questions

  * Students begin to analyze real world, IPUMS, datasets, provided by course/instructor.
  * Students begin to model real world phenomena
    + EG: SEX ~ EDUATTAIN ; SEX ~ EDATTAIN + EMPSTAT
  * Students learn to perform exploratory analysis, hypothesis testing, and statistical inference.
  * Students learn to navigate IPUMS website,and find relevant data to thier research interest.
  
**Unit 3 (Weeks 11-15):** Discussing Data and Student Research 

  * Students develop a research question to be answered with IPUMS data.
    + Students are encouraged to fit it to their interests/major/discipline.
  * Course time should be devoted to individual/small-group research.
  * Instructor/class present on recent research.
    + Instructor models constructive / scholarly criticism. 
    + Encourage students to critique published work - responsibly.




## Syllabus - Detailed {-}




### Unit 1 Understanding and Testing Data {-}

<br>
<br>

Students become gain familiarity and comfortability navigating RStudio, coding in R and performing simple data manipulation and visualization exercises. Datasets in this section consist of real-world (or synthetic) data, but the focus is on understanding data types (EG: using Age as a continuous variable; sex, education, employment as categorical; etc). Instructors should acknowledge these as **educational** datasets and make explicit trends found within these data are devoid of context, and must be taken with a (rather large) grain of salt, if at all.


By the end of Unit 1, students will be able to:

* Download R and RStudio
* Read data into R and
* Write (save) data out of R
* Summarize data visually
  + Using `base R`
  + Using `ggplot` (tidyverse)
* Summarize data in tables
  + Using base R
  + Using `gttable` / `tidyverse`
* Formally state and test assumptions of data
  + *EG:* t-test, anova, correlations, regression

By the end of Unit 1, students will understand

* Main types of data
  + *EG:* logical, numeric, character, etc
  + R specic vs general terms
* How to create and describe various data distributions
  + *EG:* normal, poisson, normal-skewed, etc
* Know which types statistical tests are appropriate for a given set of data.




#### Week 1: Intro to R, data types, data structures {-}
#### Week 2: Plotting Data, Distributions{-}
#### Week 3: Statisitcal testing of simple data sets{-}
#### Week 4: Correlation and Relationships of simple data sets {-}
#### Week 5: (TBD) {-}



### Unit 2 Finding Data and Asking Questions (Using IPUMS Data)  {-}

Here we demonstrate two **different** approaches to conducting research. Students become familiar writing up short lab reports detailing their findings. For Section \@ref(exploratory), we/instructor provides students with simple datasets from IPUMS (or other real-world data). Students will learn exploratory data analysis techniques and how to create lab reports to summarize key findings. 

For unit \@ref(hypothesis), students will learn to develop their own simple research questions or social-science hypotheses. They will seek out data to answer these questions, learning to navigate [ipums.org](https://ipums.org), and create **data extracts**, as well as hypothesis-testing statistical methods. Again, lab reports to summarize findings. 

#### Week 6: Intro to IPUMS
#### Week 7: Exploratory analysis {-}


If you've just collected a survey, or other raw data, you may not know what you're looking for. This is perfectly ok but goes against *the scientific method* most people learned in grade school.

This unit begins by presenting data/distributions and asking students to begin interpreting the data . visual exploration is encouraged and basic of data manipulation are taught
  * *EG:* how to subset data, how to reshape data, how to re-code data, how to convert from one `data type` to another.
  
Example lab exercise:

Students given a data set (xls, csv, etc)
  * load data, perform manipulations, basic summaries
    + cross tabs
    + group means by a covariate
  * inspect data visually
    + *DESCRIBE* the distribution - is it normal? significant?
  * *FIND* aquestion in the spread of the data
    + how can you test this (maybe small group work)
  * write up/ present results
    + think on confounding factors / biases

    
#### Week 8: Hypothesis Testing {-}


If, on the other hand you have an a pre-existing idea you want to test. We can follow the traditional *scientific method*. With a question in mind, the first question is: where to look. What better place than [IPUMS](https://ipums.org)!

Begin introducing navigation of web resources - mainly IPUMS international

Students should become comfortable working through lab exercises:
  * Define a question (or be presented with one)
  * Download variables from IPUMS (course downloads possible)
  * Perform a basic analysis (discussed in Unit 1)
  * Generate a **visual argument** for your analysis
    + Include explanation/interpretation/reflection on the question at hand, and the data used
    + Any obvious biases
    + Any obvious confounding factors


#### Week 9: Statistical Inference {-}
#### Week 10: (TBD) {-}





### Unit 3 Discussing Data and Student Research  {-}


Students will select their own research question that can be answered with the IPUMS data set and will spend five weeks conducting a research project complete with data analysis, visualization, and interpretation.


In this section we encourage the instructor to provide ample time for independent student/small-group research. Some class time should be devoted to modeling healthy discussion and critique of methods. Students should learn to discuss not just *how* to answer a research question but *why* they are asking/answering it. What impact does the question/answers have. Is the question releveant/meaningful, and importantly, Is this research question perpetuating racist ideas.

We provide some examples here but encourage instructors (or students) to bring in recent journal/popular articles that do (or do not) apply data science methods well.


#### Week 11: Students develop research Question {-}
#### Week 12: Students find relevant variables from IPUMS {-}
#### Week 13: Students test and evaluate results {-}
#### Week 14: Students prepare presentations of results {-}
#### Week 15: Students present work (slides, poster, podium, etc) {-}







# DEV NOTES {.unlisted .unnumbered}

### TO DO {-}

* **UPDATE TODO LIST**

* Make chapter 1 chapter 2
* Anna Adds chapter con data science intro exclusive of R/IPUMS

* discuss style
  + key terms section for each chapter?
  + key terms in **bold**
  + italics for *emphasis*
  + are we pro-hyphens, or are they pedantic?


### MISC IDEAS {-}

* Application forward
* Present research/ analysis/results FIRST, then explain the mathematical principals behind it
* daily/weekly “i’m stuck on…” 
  + Students send in questions (night before class) and instructor spends 10-15 mins talking through (or collaboratively working through with class) solutions
  + Alternatively, once a month maybe a longer class covering “common problems asked this month”
daily/weekly “recent research”
* pick out a recent article with good visualization (or bad) and spend 5-10 mins discussing what makes it good (or bad)
  + Encourage students to find articles for extra credit


### Documentation {-}

This function grabs any packages in your project and adds them to a local list that can be referenced using `R-pacakgename`
  * **NOTE** in practice, that needs to be wrapped in markdown syntax, eg:
    `[@R-bookdown]`
  * See help files for more info - might be able to create/add a `citation` file
    
```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```




Testing title fix

```{js, echo = FALSE}
title=document.getElementById('header');  
title.innerHTML = '<img src="ipums_banner.png" alt = "IPUMS Banner">' + title.innerHTML
```


<!--chapter:end:index.Rmd-->

# Unit 1: The Basics {-}

## Summary {.unlisted .unnumbered}


### Lesson 0: 

Lesson 0 files should contain a brief summary of the topics within each unit

Lesson 0 can also be used for a brainstorming space to sketch out ideas before creating `Unit#_Lesson#` files.


### Lesson 1: What IS Data / Collecting Data {.unlisted .unnumbered}
### Lesson 2: Visualizing/Describing Data {.unlisted .unnumbered}
### Lesson 3: Hypothesis Testing: Comparisons and Correlations  {.unlisted .unnumbered}
### Lesson 4: Hypothesis Testing: ANOVA and LM {.unlisted .unnumbered}
### Lesson 5: Drawing Conclusions {.unlisted .unnumbered}



### Unit-wide Glossary {-}

_Is this redundant?_



<!--chapter:end:10-Unit1_outline.Rmd-->


# WHAT IS DATA

## Engage

Brainstorm/word cloud on “what is data”


## Explore
Brainstorm “what do we DO with data”
Brainstorm/word cloud “where do we get data”
We can collect it!
  * quick poll: how many people have ___(participated in a survey? analyzed data?)__
  * XX% of this class has done ____


Pose questions to explore



## Explain 

### WHAT ARE DATA

Data are defined as “facts and statistics collected together for reference or analysis.”1 As seen in Figure 1.1, there are two types of data: quantitative and qualitative. Quantitative data are able to be expressed in numerical format and are countable. These data are either discrete or continuous where discrete data uses numeric bins. For example, we use our age as discrete quantitative data, we round our age to the previous year (eg., 20, 21, 22). Continuous data does not use bins, but rather includes all of the fractions between two whole numbers. An example could be most physical measures like height, weight, the speed at which an individual runs.
Qualitative data describe characteristics or categories and can be broken down into two categories, nominal or ordinal. Nominal data have no inherent ordering but it can be categorized. Examples include country or origin, gender, hair color, race, etc. Ordinal data can both be categorized and ordered (e.g., first, second, and third place is a race).
Going back to our hypothesis of male height on campus, heights are continuous, qualitative data. It is difficult for people to report their specific height and you assume that most individuals will report it rounded to the closest inch. This makes the data you will actually use, discrete quantitative data.





> *Fun fact: A single data point is called a datum which is Latin for “something given”. The word data can be either singular or plural depending on how you use it. It can be used as a mass noun the same way we discuss sand on a beach or hair on our head. However, in science we are usually referring to multiple datums within a data set making it a plural noun. Therefore, data is cool and data are cool!*
>
> --- Sources



### COLLECTING DATA

The first step to answering a research question is to collect your data. Broadly, data comes in two forms, primary and secondary. (Fig 1.2) Primary data are data that is collected directly by the researcher. Surveys, observations, experimentation, questionnaires, and interviews are all examples of primary data. Secondary data are collected from published or unpublished literature. It is collected by different researchers and compiled for use by a second scientist. These types of data include data found in published articles, books, journals, biographies, and government records like the US Census.


Once compiled, you now have a data set which is composed of observations and variables. An observation is all of the measures taken for one person or item. A variable is what is being measured.


The US CDC data is secondary, but you are collecting height data yourself in class as a comparison. The survey or questionnaire you use on your classmates is primary data. Each individual is an observation and the variable of interest is height.


#### "HOW do we get data??"

The first step to answering a research question is to collect your data. Broadly, data comes in two forms, primary and secondary. (Fig 1.2) Primary data are data that is collected directly by the researcher. Surveys, observations, experimentation, questionnaires, and interviews are all examples of primary data. Secondary data are collected from published or unpublished literature. It is collected by different researchers and compiled for use by a second scientist. These type of data include data found in published articles, books, journals, biographies, and government records like the US Census.
Once compiled, you now have a data set which is comprised of observations and variables. An observation is all of the measures taken for one person or item. A variable is what is being measured.
The US CDC data is secondary, but you are collecting height data yourself in class as a comparison. The survey or questionnaire you use on your classmates is primary data. Each individual is an observation and the variable of interest is height.




#### TYPES OF DATA

There are a lot of different ways to record observations. Its important to choose an appropriate format to record your data. Some pretty broad categories are:

* Yes/No
* Tallies
* Categorization
* Measurments
* Open-ended text

If you were to ask your friend: "What is your favorite food?" You would not expect
them to give an answer of "yes" or "no". Asking an open-ended question often/always prompts an open-ended response. This can be informative and inclusive, allowing respondents to answer with exactly the answer they feel best answers the question. However, it can make analyzing and drawing interpretations from the data dificult to impossible!

If instead, we ask "Is your favorite food pizza?" We expect a yes/no answer. Asking this question of 100 people, we can easily answer the question "What percentage of people's favorite food is pizza?"

##### Continuous vs categorical

One of the biggest differences in classifying data is based on the uniuque values we expect from the data. Continuous variables are...

Categorical variables can be ordered (EG Factors, ordinations) or they can be undoredered (EG, categories)

###  POPULATIONS AND SAMPLING


**Random Sampling:** It is a sampling method in which all the items have an equal chance of being selected and the individuals who are selected are just like the ones who are not selected

**Stratified Random Sampling:** It is a process to gather data by separating the actual population into the distinct subset or strata, and then choosing simple random samples from each stratum Your research question is about the height of all males at your college but recording height data for each individual would be very difficult and time consuming. You instead decide to use a sample of males in your data science class. This is a random sample as each male individual has an equally likely chance of being samples (that is, unless a prerequisite exists).


Sampling strategy can lead to bias. **Statistical bias** is a systematic tendency which causes differences between results and facts. If instead of your classmates, you had chosen a different sample, like the men’s basketball team, your results would have been biased as basketball players are taller on average.



#### Sample vs Population

Is the study sample a representative sample of the population? 

#### How to draw samples

Random Sampling: It is a sampling method in which all the items have an equal chance of being selected and the individuals who are selected are just like the ones who are not selected
Stratified Random Sampling: It is a process to gather data by separating the actual population into the distinct subset or strata, and then choosing simple random samples from each stratum Your research question is about the height of all males at your college but recording height data for each individual would be very difficult and time consuming. You instead decide to use a sample of males in your data science class. This is a random sample as each male individual has an equally likely chance of being samples (that is, unless a prerequisite exists).
Sampling strategy can lead to bias. Statistical bias is a systematic tendency which causes differences between results and facts. If instead of your classmates, you had chosen a different sample, like the men’s basketball team, your results would have been biased as basketball players are taller on average.






### Study Design Considerantions - Bias 

is there bias in the sampling?
is there bias in the data types collected?

If so, Be explicit 
Our classroom represents a subset of individuals in this country:
college aged
attending college
specific geography 
specific time period

we group categories a,b,c to make new groups for analysis.

In doing so, we limit our interpreations to _____



### EXPLORATORY DATA ANALYSIS


The first step in understanding and interpreting our data is called an exploratory data analysis. We will use a few measurements to quickly look at the data and then we can use some simple graphing techniques to turn our data into visualizations. The first three M’s, you are likely familiar with and are often referred to as measures of central tendency: Mean, Median, and Mode. 
These go along with range, outliers and sample size. 


  * Mean
  * Median
  * Mode
  * Outlier
  * Range


>
> What is a statistic?
>



### Exploring height

If you do not have class data on height, we will be using the following simple dataset of 5 individuals:

* Can you describe what is happening in the following `codechunk`??
* What do `person, height` represent in relationship to `ex_height`??
 
<details>
 <summary> *Click to show answer* </summary> <br> 
  <p>
  > We **create** the **R object** `ex_height` using the **assignment operator** `<-`
  > `ex_height` is a **data.frame**, a table, with two columns: `person` and `height` <br>
  >  `person, height` are the two columns, or variables, of `ex_height`
  >
  
  </p>
</details>

 
```{r}

ex_height <- data.frame(
  "person" = paste("Ind", 
                   c("a", "b","c","d","e"), 
                   sep = "_"),
  "height" = c(5.5, 5, 6, 5.25, 5)
)


```



```{r, results = 'asis'}


knitr::kable(ex_height)
```


### Mean height

You probably already know this one. The (arithmatic) mean is calculated by adding all **values** together, and dividing by the **number of observations**. For our dataset, we add all 5 heights together and divide by the number of individuals (5):

$\frac{(5.5 + 5 + 6 + 5.25 + 5)}{5}$  
$\frac{26.75}{5}$  
$5.35$

In `R`, we can write this out "by hand." Since `R` is a **statistical programming language**, r recognizes basic mathematical expressions. We can `code` the following to calcualte the mean:


```{r}

(5.5 + 5 + 6 + 5.25 + 5)/5

```



Since `R` is an **object-oriented programming language**, we don't need to write out individual numbers for each calculation. Instead, we can refer to the `ex_height` **object**. Since `ex_height` is a **data.frame**, we can refer to it variables by name with the `$` operator. We also take advantage of some of the built-in mathematical functions of `R`: `sum(), length()`

```{r}

sum(ex_height$height)/length(ex_height$person)

```

### Median height

The **median** is calculated by ordering all values from small to large. If there are an **odd** numberof values, there will be a single value at the middle. 

Our original data:


5.5, 5, 6,  5.25,  5

Our data re-ordered from small to large, then we simply cross off values from either end to find the middle value:


5, 5, 5.25, 5.5, 6


~~5~~, 5, 5.25, 5.5, ~~6~~


~~5~~, ~~5~~, 5.25, ~~5.5~~, ~~6~~ 



If we had an even number of values, we would wind up with two "middle values", in which case we take the mean of these two. 

If we had one more value, let's say `5.75`, we wind up with both `5.25` and `5.5` as middle values. 


5, 5, 5.25, 5.5, 5.75, 6

~~5~~, 5, 5.25, 5.5, 5.75, ~~6~~

~~5~~, ~~5~~, 5.25, 5.5, ~~5.75~~, ~~6~~


The median is the **mean** of these two values:



$\frac{5.25+5.5}{2}$

$5.375$


In a small dataset, it's easy to pick out the middle value. Fortunately, there's an `R` function for this as well:


```{r}

## With 5 Individuals 
median(ex_height$height)

## With 6 individuals
median(c(ex_height$height, 5.75))
```


>
> Did you notice the `c()` function above? What does it do?
>



### Modal height

The **mode** is the most common value in the dataset. Here again, it's easy to pick out there are 2 people with a `height` of `5`, and all other values are represented by just one person. There's actually no build-in `R` function to calculate the **mode**...


But don't worry, there are functions that let you decide the **mode** and more!! A very useful function is `table()`, used to make **counts** of values. In a small dataset, we see that two individuals have a height of 5. Our **mode** is 5!

```{r}

table(ex_height$height)

```



### Range

The **range** of the data is two numbers, the lowest, and highest values within the data:

```{r}

range(ex_height$height)

```


Though, if you want just the **minimum** or **maximum** value, you can use `min()` or `max()`:

```{r}

min(ex_height$height)
max(ex_height$height)
```




### Outliers

So far, some of these examples may have so obvious you may be thinking: *what's the point??*

For a slightly more practical demonstration, try out the above functions on your `class_data`. You can also access our sample dataset, curtousey of, [IPUMS-Health Surveys](nhis.ipums.org). The heights and weights dataset is included in the accompanying `ipumsED` R package. 

There are a few ways to actually get your data into `R`, depending on the file type your data is saved as. If it is a `.csv`, you can use the base-R function `read.csv()`. If you have data in an Excel file (.xls, .xlsx) you will need install the `readxl` R package in order to use the add-on function of `read.xlsx()`.

>
> Notice the simliarity between `read.csv()` and `read.xlsx()??`
> While packages are created by the R community, they often try to maintain syntanx/coding conventions from base R.
>


In order to keep things organized, we will first create an object called `my_data_path` that contains the **file path** to the data I want to read. You can type this out as a single character string, or use the `file.path()` function, which adds in `/`s for you. Next we **pass** our new object, `my_data_path`, as the **argument** to either `read.csv()` or `read.xlsx` 


```{r}
## for now

my_data_path <- file.path("..", "ipumsED", "data", "nhis_sample.csv")


my_data <- read.csv(my_data_path)

```


If you have the `ipumsED` package installed, you can load the dataset into your local environment simply by using the `data()` function. 


```{r, eval = FALSE}
data(nhis_data)
```


We can inspect our data object with some basic functions. Notice how the output differs when the function is called on a `data.frame` versus when it is called on a `vector`.


```{r}
class(my_data)
summary(my_data)

class(my_data$HEIGHT)
summary(my_data$HEIGHT)

```


Calling the R object itself usually prints the contents. For something this big, that's probably inadvisable, so we print just the first 10 records using the `head()` function. 

>
> There's also a `tail()` function for printing the last 10 recods
>

```{r, eval = FALSE}
my_data
```

```{r}

head(my_data)
```
### Find Outliers

1000 records is probably too much to review manually (and who would want to!?). Instead, we can **visualize** our data as one method to detect outliers. a **box-and-whisker plot** or **boxplot** is a common graph to plot continuous data like height.


Box-and-whisker plots, by default, show you which values are outliers! The box represents the middle 75% of the dataset. The whiskers account for another 11% on either end of the box. The **median** is represented by a thick black line. Any observations outside the whiskers are considered outliers


```{r}
boxplot(my_data$HEIGHT, main = "Height in Inches")

```


In this case, the values of 0, 95,96,97,98,99 all represent special coded values. `0` Indicates someone Not In Universe - someone for whom this question wasn't asked. This may be an infant too young to meaningfully be measured. In this case higher values represent a value of `unknown` for various reasons. Both of these values represent missing data, but they mean different things.

>
> Why do you think we have more than one value to represent missing data? How does a value of 0 compare to a value of 99??
>  

We may want to exclude these special cases from our analysis. We can easily perform a data modification by filtering out values that equal any of our special cases. You can see the **range** of the graph is smaller, though visually, the box and whiskers are larger. Do we still see outliers in height?

```{r}
library(dplyr)

new_data <- my_data %>% filter(HEIGHT > 0 & HEIGHT < 95)
boxplot(new_data$HEIGHT)

```




## Elaborate

### Your Turn! 

For this example, you can use data collected in class, or the example dataset available in `ipumsED` R package: `class_ipums_data`

For in-class *survey* data, you are looking at the age of each individual in class. 
For the IPUMS data, you are looking at age as recorded in the American Community Survey
([ACS](https://www.census.gov/programs-surveys/acs)). This survey is fielded 
annually. It contains more detailed questions that supplement the decennial census. **Raw data** are available directly from the [US Census Bureau](https://www.census.gov/programs-surveys/acs/data.html). However, we will be working with [IPUMS data](https://ipums.org), **harmonzied.**. This process standardizes variable coding schema across all samples within the IPUMS dataset. IPUMS makes it easy for researchers to conduct analysis across time and space. 

> IPUMS began as a PhD project digitizing historic US census data and reconciling
> it with the coding schema of the current US Census (1990s). IPUMS-International grew out
> of this by employing our harmonization practices on Census samples from other countries.
> IPUMS has partnerships with over 100 countries and is working on building partnerships with the 
> remainng **82** countries!
>
> Partnerships with IPUMS may look slightly different for each country, but they always
> involve IPUMS commitment to being responsible stewards of the partner-country's data.
> Our goal is to make data as open as possible, while maintaining responsible disclosure risk controls.


Example data used throughout this book come from IPUMS-International so that values may be contextualized for a readers interested in one of the 103 countries that have data sharing agreements with IPUMS.



Write and `Rscript` that reads in your chosen dataset, and calculates the 
following statistics:

* `mean()`
* `meadian()`
* `mode`
* `range()`

In addition, include at least one plot or table and use it to discuss if your dataset contains any outliers. What criteria are you using to define an outlier?


> Where's the `mode()`?? As a statistical programing language, R was developed with
> built-in functions for `mean()`, `median()`, and `range()`. But `mode` wasn't important
> enough to warrant its own function. In part, because it's so easy to code, even you
> can do it!
>
> Yep, we're serious!



## Evaluate/Exercises


### Context:

Using the data set from Section 1.4 and the R statistics you produced, 
answer the following questions about our data set.
 
### Questions:

  
**Are the data quantitative or nominal?**

<details>
 <summary> *Click to show answer* </summary> <br> 
  <p> 
  
  > 
  > Age data is quantitative.
  >
    
  </p>
</details>




**Are the data discrete or continuous?**

<details>
  <summary> *Click to show answer* </summary> <br> 
  <p> 
  
  >
  > Age data is could be continuous depending on how you measure and record it, 
  but the way we generally talk about 'Years of Age' is actually discrete.
  >
  
  
  </p>
</details>




**Is the dataset primary or secondary data?**

<details>
  <summary> *Click to show answer* </summary> <br> 
  <p> 
  > That depends on which data you are using. If it was collected in your class, it is primary data. If you are using the IPUMS data, it is secondary data as the information was collected by IPUMS from the US Census.
  
  </p>
</details>




**What sampling method was used?**

<details>
 <summary> *Click to show answer* </summary> <br> 
  <p> 
  
  > 
  > Age data is quantitative.

  </p>
</details>




**Does any statistical bias exist?**

<details>
 <summary> *Click to show answer* </summary> <br> 
  <p> 
  > 
  > Age data is quantitative.
  
  </p>
</details>




**Which measure of central tendency is best to describe this data? Mean, Median or Mode.**

<details>
 <summary> *Click to show answer* </summary> <br> 
  <p> 
  >
  >Age data is quantitative.
  
  </p>
</details>




## GLOSSARY


Quantitative Data
Discrete Data
Continuous data:
Nominal Data: 
Ordinal Data:
Primary Data:
Secondary Data:
Data Set:
Observation:
Variable:
Random Sampling:
Stratified Random Sampling:
Statistical Bias:
Mean:
Median: 
Mode:
Range:
Outlier:
Statistic:
Sample Size:



## Additional Content

#### examples from recent academic articles

article graphs, citations, instructors research, etc

#### examples from non-academic

customer satisfaction; workplace/department culture; r&D; 


<!--chapter:end:11-Unit1_what_is_data.Rmd-->

# Visualizing Data

**CURRENTLY JUST A COPY OF THE OLD 1.2, what is R; needs to be updated **

## Engage {.tabset}

show two data viz side by side and ask best for the data. 

### Vis 1

**Pose a question**



<details>
 <summary> *Click to show answer* </summary> <br> 
  <p> 
  
  > 
  > ANSWER HERE
  >
    
  </p>
</details>



### Vis 2

### Vis 3




## Explore

What do you want it to show vs what it shows. Maybe find a real data image from news that is clearly not good visualization

## Explain - lecture/read

### Single Variable Visualization


* Histogram,
* Box Plot, 
* bar graphs,
* pie charts, etc.

### Multi variable visualizations

 * scatter plots, line graphs
 
 
### Data Tables



## Elaborate

### Making visualizations in R
Histograms:
Box Plots:
Bar Graphs:
Pie Charts:
Scatter Plots:
Line Graphs:
Tables:
More: ______



### Which visualization is best for my data? Things to consider

List things to consider for a visualization



## Evaluate/Exercises

Give them a data set and have them think through why they choose a specific visualization. Have the create and answer questions about it.
We will go through the list of possible visualizations for the same data and explain which work, which dont, and why


<!--chapter:end:12-Unit1_Visualizing_Data.Rmd-->


# Describing Data

**see old_ for some examples, not very exstenive**

## Engage

post-it note histogram of height:
  * one with MANY bins
  * one with FEW bins
  * *do these graphs tell the same story??*



## Explore

How do we describe this meaningfully??
can we determine the "average"



have class guess at average (likely the heighest point of histo)
  * Interrogating a commonly held assumption
    + the average height of humans is 5'5"


Pose questions to explore

what does "average" mean??



## Explain - lecture/read

### How do we describe data - summary statistics
### How do we describe the distribution of data 
### How do we manipulate data 
#### coding/categorizing free response/ “binning” a continuous variable

Height, while we treat as continuous, is actually "binned" into ~60 unique values.

## Normality

What does it mean to be normally distributed. (how measures of central tendency compare)
discuss skew / kurtosis
specialized vocabulary to describe data
Why people care about normality (what tests can/t you use and why)


## Elaborate

Read/write data
Visualize data - describe the distribution; compare to board results
Tabulate Data - describe with summary statistics 
Test of normality: one-way t-test ; QQ plot ; shapiro-wilks (note high rate of rejection)
Demo binnig of ages



## Evaluate/Exercises

Write an R script that: 
1. reads in data
1. tests normality across variables
1. print results as table
1. prints results as graph.

Reflect: Are any variables non-normal? Do you have any interpretations as to why?
Do you have any questions to test

<!--chapter:end:13-Unit1_describing_data.Rmd-->

# Hypothesis Testing - Comparison and Correlations


## Engage

Post-it histogram - height separated by birth month (may require a lot of board space)

Are the distributions similar? different? **too many small samples to tell**?



## Explore

How do we **group** data in order to compare it.

The most simple question in statisitics is:
Is x bigger than y

How can we do that for our groups here?

Summer/Winter?

conduct demo/small-scale experiment/analysis
Ask leading questions, form hypotheses; 
  * can the example be grouped / broken into subgroups
    + does the same pattern/phenomenon apply?
  * Is there a corollary / inverse phenomenon?

Pose questions to explore



Discuss/interrogate the pattern of the data
  * does the shape imply anything
  * try to have it student led / guided
  * with time, students guess at what the data shows/doesn't show without labels



## Explain - lecture/read

demo recode: birth month -> Summer/Winter (daylight-savings or not); 
recode favorite food to sweet/savory

### Why 2 groups?

Historical problem of statistics. We've developed very good methods for very un-reasonable
situations. In order to make these tests work, we often have to **manipulate**
our data in order to meet the **assumptions** of the specific test or analysis
that we'd like to perform.

### Common tests to compare 2 groups
t-test and chisq | box-whisker, histogram, density distribution

### Correlation(s)
correlation (pearson, spearman) | scatter plot
height ~ hrs_slept ; height ~ fav_food_category




## Elaborate
demo recode: birth month -> Summer/Winter (daylight-savings or not); recode favorite food to sweet/savory
t-test and chisq | box-whisker, histogram, density distribution
height ~ birth_SW
correlation (pearson, spearman) | scatter plot
height ~ hrs_slept ; height ~ fav_food_category
What is an hypothesis??




## Evaluate/Exercises

Formulate a hypothesis and test it using class survey data.
Recode birth month to birth_quarter; recode favorite food to 3 or more categories

<!--chapter:end:14-Unit1_comparing_correlations.Rmd-->


# Hypothesis Testing - ANOVA and LM


## Engage
Compare MORE than 2 groups

Post-it histogram - height separated by Birth Mo (reclassified to birth quarter)



## Explore

Pose questions to explore

Are the four distributions similar? different?





## Explain - lecture/read


What is ANOVA, how to interpret tabularly, graphically (box-whisker, histo, density); non-parametric alternative (KS)
What is a linear model? LM vs Correlation vs ANOVA
What does a linear relationship imply?
LM = continuous ~ continuous
ANOVA = continuous ~ categorical

## Elaborate

Conduct recoding for birth quarter
conduct ANOVA
Conduct Linear Model


Work through the same process/analysis on new data. Either a different 
variable(s) in the same sample, or applying/demonstrating the phenomenon 
using real-world data/examples.


* Reiterate/reinforc the process/task of the lesson
* Does this fit in to other steps/analysis
* does this fit in to other thematic topics?

(Possibly) Build on the first one to show variation.
  * Other ways to represent the phenomenon/data
    + tabular
    + visually



## Evaluate/Exercises

Formulate a hypothesis and test it using class survey data.

<!--chapter:end:15-Unit1_comparing_data.Rmd-->

# Unit 2: IPUMS {-}

## Lesson 6 Introduction to IPUMS {.unlisted .unnumbered}



Some text to break up the sub-section headers

### Intro to IPUMS website {-}
### background on ipums {-}
### navigating website {-}

Find certain (very common) variables to answer (common) social science questions.


## Lesson 7 Exploratory analysis {.unlisted .unnumbered}

If you've just collected a survey, or other raw data, you may not know what you're looking for. This is perfectly ok but goes against *the scientific method* most people learned in grade school (More on that to follow(**_include_link_**)).

This unit begins by presenting data/distributions and asking students to begin interpreting the data . visual exploration is encouraged and basic of data manipulation are taught
  * *EG:* how to subset data, how to reshape data, how to recode data, how to convert from one `data type` to another.
  
Example lab exercise:

Students given a data set (xls, csv, etc)
  * load data, perform manipulations, basic summaries
    + cross tabs
    + group means by a covariate
  * inspect data visually
    + *DESCRIBE* the distribution - is it normal? significant?
  * *FIND* aquestion in the spread of the data
    + how can you test this (maybe small group work)
  * write up/ present results
    + think on confounding factors / biases


### Advanced Exploration - Change Over Time {-}

Here we demonstrate an approach to looking at how Family Structure (inferred from household relationships) has changed over time.


#### Setup / Load Data {-}

Install/update R packages

```{r, eval=FALSE}

install.packages("ipumsr")
install.packages("tidyverse")
```


Data extract created online using the datacart system.

```{r, eval = FALSE}

library(ipumsr)
library(dplyr)


ddi <- read_ipums_ddi("Data/ipumsi_00005.xml")
data <- read_ipums_micro(ddi)
```


##### Inspect the Data {-}

Using `haven` labeled values. 

```{r, eval = FALSE}

data$RELATE[1:100]
class(data$RELATE)

data %>% count(RELATE)
data %>% count(SEX)
```

What were those codes ??

```{r, eval = FALSE}

## need to convert this to an image or something similar; kable table?
ipums_view(ddi)

```


##### Visualize {-}

A simple plot

```{r, eval = FALSE}

plot(AGE ~ YEAR, data = data)

```

A fancier plot


```{r, eval = FALSE}

plot(AGE~YEAR, data = data, type = "n", main = "Age by Sex, over Time, CO")
points(data$YEAR[data$SEX==1]-1, data$AGE[data$SEX==1], pch = 16, col = hsv(.6,.6,.8,.2))

points(data$YEAR[data$SEX==2]+1, data$AGE[data$SEX==2], pch = 16, col = hsv(1,.6,.8,.2))

abline(lm(AGE~YEAR, data = data), col = "green")

```


##### Asking (logical) questions {-}

Here we demonstrate how setting up logical questions can be used to easily filter/subset data.

```{r, eval = FALSE}

age_test <- data$AGE > 18

class(age_test)

age_test

```

Logical vectors are stored as `TRUE` or `FALSE`, but can also be evaluated numerically as `1` or `0` respectively. We can therefore `sum()` the number of `TRUE` values and divide by total rows for a proportion.

```{r, eval = FALSE}

sum(age_test)/nrow(data)

```



##### HH vs persons {-}

A unique characteristic of census and some survey data is the nested-structure with individuals being grouped into households. Often times it is necessary to choose to work at the hh or person level, and data must be appropriately manipulated to fit that case.

```{r, eval = FALSE}

hh_total <- length(unique(data$SERIAL))
hh_total
ipums_view(ddi)
```


#### Nuclear Family {-}

First we look at a nuclear family, comprising only parents and their immediate children. 

```{r, eval = FALSE}

library(ipumsr)
library(dplyr)

ddi <- read_ipums_ddi("/pkg/ipums/personal/ehrli097/AABA_2022/Data/ipumsi_00005.xml")
all_data <- read_ipums_micro(ddi)

census_years <- c(1860, 1870, 1880, 1900, 1910, 1960, 1970, 1980, 1990, 2000, 2010)

## subset census only
d2 <- all_data %>% filter(YEAR %in% census_years)

## make a household dataframe
hhs <- d2 %>% distinct(YEAR, SERIAL, .keep_all = TRUE) %>% select(YEAR,SERIAL,GEO1_US)

hhs %>% View()

```




```{r, eval = FALSE}

hhs <- d2 %>% filter(RELATE ==4) %>% 
  
  
  distinct(YEAR, SERIAL) %>% mutate(extended_test=TRUE) %>% right_join(hhs, by = c("YEAR", "SERIAL")) %>% mutate(extended_test=if_else(is.na(extended_test),FALSE,TRUE))
  








hhs <- d2 %>% filter(!RELATE %in% c(1, 2, 3) |
                  (RELATE == 3 &
                     MARST %in% c(2, 3, 4))
                ) %>% 

  
  
  distinct(YEAR, SERIAL) %>% mutate(nuclear_test = FALSE) %>% right_join(hhs, by = c("YEAR", "SERIAL")) %>% mutate(nuclear_test = if_else(is.na(nuclear_test), TRUE, FALSE))



table(hhs$extended_test,hhs$nuclear_test)


```


##### Tabulate results  {-}


```{r, eval = FALSE}
  hhs <- d2 %>% filter(RELATED %in% c(4200, 4210, 4211, 4220, 4500, 4510, 4600)) %>% distinct(YEAR, SERIAL) %>% mutate(parent_test=TRUE) %>% right_join(hhs, by = c("YEAR", "SERIAL")) %>% mutate(parent_test=if_else(is.na(parent_test),FALSE,TRUE))


  hhs <- d2 %>% filter(RELATED %in% c(4100, 4110, 4120, 4130, 4300, 4301, 4302)) %>% distinct(YEAR, SERIAL) %>% mutate(children_test=TRUE) %>% right_join(hhs, by = c("YEAR", "SERIAL")) %>% mutate(children_test=if_else(is.na(children_test),FALSE,TRUE))
  
  
  res_tabs <- list(
    "nuclear_test" = hhs %>% group_by(YEAR, nuclear_test,GEO1_US) %>% summarize(.groups="drop",n = n()) %>% as.data.frame(),
  "extended_test" = hhs %>% group_by(YEAR, extended_test, GEO1_US) %>% summarize(.groups="drop",n = n()) %>% as.data.frame(),
  "parent_test" = hhs %>% group_by(YEAR, parent_test, GEO1_US) %>% summarize(.groups="drop",n = n()) %>% as.data.frame(),
  "children_test" = hhs %>% group_by(YEAR, children_test, GEO1_US) %>% summarize(.groups="drop",n = n()) %>% as.data.frame()
  )
  
  
  
  

collapsed_results <- res_tabs %>% purrr::map(function(x){
  x <- x %>% group_by(across(names(x)[1:3])) %>% summarize(.groups="drop",n = sum(n))

})


collapsed_results <- lapply(collapsed_results, function(x){
  colnames(x)[2] <- "test"
  colnames(x)[3] <- "state"
  return(x)
})

combined <- collapsed_results %>% purrr::reduce(full_join, by = c("YEAR", "test", "state"))



colnames(combined) <- c("YEAR","test", "state", "n_nuclear", "n_extended", "n_parent", "n_children")

combined[is.na(combined)] <- 0


to_plot <- combined %>% group_by(YEAR, state) %>% mutate(n_tot = sum(n_nuclear)) %>% ungroup() %>% mutate(pct =  across(starts_with("n_"))/n_tot) %>% select(YEAR,test,state,pct)

```


##### Visualize Nuclear Families {-}

```{r, eval = FALSE}
to_plot <- to_plot %>% filter(test==TRUE)



plot(to_plot$YEAR, to_plot$pct$n_nuclear, col = hsv(.4, .6,.8), pch = 16, ylim =c(0,1), xlab = "", ylab = "prop of hhs", main = "Nuclear HHs over time in CO")


```




#### Extended Family {-}


Next we look at hhs with extended families present. IE, any that contain more relationships than just Parent/Child/Sibling (between children only)


##### Gernerate models {-}

```{r, eval = FALSE}


to_plot <- to_plot %>% filter(test==TRUE)


glm_hist <- glm(pct$n_extended ~ YEAR, data = to_plot[to_plot$YEAR < 1950,], family = quasibinomial(link=logit))



glm_hist_x <- seq(from=1860, to = 1910, length.out = 100)
glm_hist_y <- predict(glm_hist, list(YEAR = glm_hist_x), type = "response")

glm_mod <- glm(pct$n_extended ~ YEAR, data = to_plot[to_plot$YEAR> 1950,], family = quasibinomial(link=logit))

glm_mod_x <- seq(from = 1960, to = 2010, length.out = 100)
glm_mod_y <- predict(glm_mod, list(YEAR = glm_mod_x), type = "response")

mods <- list("hist"=list(),
             "mod" = list()
             )
mods_plots <- list("hist"=list(),
                   "mod" =list()
                   )

for(i in names(to_plot$pct)){
  
  hist_x <- to_plot$YEAR[to_plot$YEAR < 1950]
mod_x <- to_plot$YEAR[to_plot$YEAR > 1950]

  mods$hist[[i]] <- lm(pct[[i]] ~ YEAR, data = to_plot[to_plot$YEAR < 1950,])
  
  mods_plots$hist[[i]] <- 
    data.frame("x" = hist_x,
               "y" = predict(mods$hist[[i]], 
                             list(YEAR =hist_x), 
                             type = "response")
               )
  
  
  
  mods$mod[[i]] <- lm(pct[[i]] ~ YEAR, data = to_plot[to_plot$YEAR > 1950,])
  
  
  mods_plots$mod[[i]] <- 
    data.frame("x" = mod_x,
               "y" = predict(mods$mod[[i]], 
                             list(YEAR =mod_x), 
                             type = "response")
               )
}



```



##### Visualize {-}

```{r, eval = FALSE}


plot(to_plot$YEAR, to_plot$pct$n_extended, col = hsv(.95, .6,.8), pch = 16, ylim =c(0,.25), bg = "grey", xlab = "", ylab = "pct of hhs with extended family")


lines(glm_hist_x,glm_hist_y, col = hsv(.95, .3, 1), lwd = 2)
lines(glm_mod_x, glm_mod_y, col = hsv(.95, .3, 1), lwd = 2, lty = 2)




points(to_plot$YEAR,
       to_plot$pct$n_extended,
       pch = 23,
       bg = hsv(.95,.6,.8))
```


#### Even more DETAIL - maybe remove {-}

```{r, eval = FALSE}

ipums_view(ddi)

```


```{r, eval = FALSE}


  
  hhs <- d2 %>% filter(RELATED %in% c(4200, 4210, 4211, 4220, 4500, 4510, 4600)) %>% 
  
  distinct(YEAR, SERIAL) %>% mutate(parent_test=TRUE) %>% right_join(hhs, by = c("YEAR", "SERIAL")) %>% mutate(parent_test=if_else(is.na(parent_test),FALSE,TRUE))

  hhs <- d2 %>% filter(RELATED %in% c(4100, 4110, 4120, 4130, 4300, 4301, 4302)) %>% distinct(YEAR, SERIAL) %>% mutate(children_test=TRUE) %>% right_join(hhs, by = c("YEAR", "SERIAL")) %>% mutate(children_test=if_else(is.na(children_test),FALSE,TRUE))
  

```


##### Parents Supporting Parents {-}

```{r, eval = FALSE}



plot(to_plot$YEAR, to_plot$pct$n_extended, col = hsv(.95, .6,.8), pch = 16, ylim =c(0,.25), bg = "grey", xlab = "", ylab = "pct of hhs with extended family")


lines(glm_hist_x,glm_hist_y, col = hsv(.95, .3, 1), lwd = 2)
lines(glm_mod_x, glm_mod_y, col = hsv(.95, .3, 1), lwd = 2, lty = 2)


lines(mods_plots$hist$n_parent,col = hsv(.8, .3,1), lwd = 2)

lines(mods_plots$mod$n_parent, col = hsv(.8, .3,1), lwd = 2, lty = 2)

points(to_plot$YEAR,
       to_plot$pct$n_parent,
       pch = 23,
       bg = hsv(.8, .6,.8))


points(to_plot$YEAR,
       to_plot$pct$n_extended,
       pch = 23,
       bg = hsv(.95,.6,.8))

```



##### Parents Supporting (extended) children {-}

```{r, eval = FALSE}

plot(to_plot$YEAR, to_plot$pct$n_extended, col = hsv(.95, .6,.8), pch = 16, ylim =c(0,.25), bg = "grey", xlab = "", ylab = "pct of hhs with extended family")


lines(glm_hist_x,glm_hist_y, col = hsv(.95, .3, 1), lwd = 2)
lines(glm_mod_x, glm_mod_y, col = hsv(.95, .3, 1), lwd = 2, lty = 2)



lines(mods_plots$hist$n_children, col = hsv(.55,.3,1), lwd = 2)


lines(mods_plots$mod$n_children, col = hsv(.55,.3,1), lwd = 2, lty = 2)


points(to_plot$YEAR,
       to_plot$pct$n_children,
       pch = 23,
       bg = hsv(.55,.6,.8))

points(to_plot$YEAR,
       to_plot$pct$n_extended,
       pch = 23,
       bg = hsv(.95,.6,.8))
```






## Lesosn 8: Hypothesis Testing {.unlisted .unnumbered}


If, on the other hand you have an a pre-existing idea you want to test. We can follow the traditional *scientific method*. With a question in mind, the first question is: where to look. What better place than [IPUMS](https://ipums.org)!

Begin introducing navigation of web resources - mainly IPUMS international

Students should become comfortable working through lab exercises:
  * Define a question (or be presented with one)
  * Download variables from IPUMS (course downloads possible)
  * Perform a basic analysis (discussed in Unit 1)
  * Generate a **visual argument** for your analysis
    + Include explanation/interpretation/reflection on the question at hand, and the data used
    + Any obvious biases
    + Any obvious confounding factors


## Lesson 9: Statistical Inference {.unlisted .unnumbered}
## Lesson 10: (TBD) {.unlisted .unnumbered}


We describe our methods in this chapter.

Math can be added in body using usual syntax as follows. This may be useful, particularly for explaining the math side of things. 


<!--chapter:end:20-Unit2_outline.Rmd-->

# Unit 3: Independent Research {-}


Students will select their own research question that can be answered with the IPUMS data set and will spend five weeks conducting a research project complete with data analysis, visualization, and interpretation.


In this section we encourage the instructor to provide ample time for independent student/small-group research. Some class time should be devoted to modeling healthy discussion and critique of methods. Students should learn to discuss not just *how* to answer a research question but *why* they are asking/answering it. What impact does the question/answers have. Is the question releveant/meaningful, and importantly, Is this research question perpetuating racist ideas.

We provide some examples here but encourage instructors (or students) to bring in recent journal/popular articles that do (or do not) apply data science methods well.

## Lesson 11: Students develop research Question {.unlisted .unnumbered}
## Lesson 12: Students find relevant variables from IPUMS {.unlisted .unnumbered}
## Lesson 13: Students test and evaluate results {.unlisted .unnumbered}
## Lesson 14: Students prepare presentations of results {.unlisted .unnumbered}
## Lesson 15: Students present work (slides, poster, podium, etc) {.unlisted .unnumbered}

By this point, students should be familiar with basic concepts from Chapter Unit 1. These include:

* Basic Coding
  + read/write data in/out of R
  + basic manipulations
* Theoretical Basis
  + looking at data distributions
  + formal assessment of distributions
  
Students will also be familiar with how these concepts are applied from Chapter Unit 2. Hopefully students will be able to:

* Come up with a social science question they are interested in
  + Critically think about target variable(s) of interest. Any *a priori* covariates? confounders?
  + Acquire relevant data from IPUMS
  + Analyze, Summarize, Visualize Data
    + scope and complexity at student/teach discretion
  + Present research to class
    + **potentially** critically discuss/evaluate each others work. 
    + **science is collaborative** everyone should be out to do their best work and represent the data as best we can. We all have conscious and unconscious biases, and the best way to confront them is share and receive (respectful) feedback.
    
During this Unit, we suggest giving ample class time for independent student research, peer-to-peer collaboration, and basic R/stats troubleshooting. This would also be a great time to model how to give respectful criticism by discussing recent research papers. 
  * We could maybe come up with 1-2 seed examples, with a few talking points

### Example one

### Example two

<!--chapter:end:30-Unit3_outline.Rmd-->

# Example RMD code {#ex_code}
 
For now, this chapter is a bit of a placeholder. I'm not sure what/how the `references.Rmd` file actually fits in to the code/construction (it looks automatic) so I want to keep that in place and need a section to note that. 

I also want a more centralized reference point to put any example code I find helpful while working in R/bookdown. This section could get really unrully really fast, but oh well.



## Core

`index.Rmd` is required and treated as file `00`. Chapters *should* be numbered for ease of sorting but custom orders are possible by specifying filenames somewhere **in this file**


Remember each Rmd file contains one and only one chapter, and a chapter is defined by the first-level heading `#`.
  + **IE** beyond the YAML header this file functions as a normal chapter since it starts with a top level header.
  + Note that `index.Rmd` has its own YMAL in addition to the various .yml files...not sure exactly how these relate.

Reference a figure by its code chunk label with the `fig:` prefix, e.g., see Figure \@ref(fig:norm_dist_plot). Similarly, you can reference tables generated from `knitr::kable()`, e.g., see Table \@ref(tab:norm_summary_tab).
  * Again, this prints an auto-generated numeral
  * also leaving this in the context of the plots in Chapter \@ref(unit2)
  

You can write citations, too. See `knitr::write_bib()` for more on this. Quick example from demo/index (may not work without write_bib() though): we are using the **bookdown** package [@R-bookdown] in this sample book, which was built on top of R Markdown and **knitr** [@xie2015].
  * If included, "Refernces" section gets added to each chapter. 
  * Not exactly sure where


Embed html renders (EG, fancy tables (IPUMS_var_desc), or any shiny app) with `webshot` R package and `phantomJS`.

```{r, eval = FALSE}
install.packages("webshot")
webshot::install_phantomjs()
```
  
  
Embed figures from a folder.

For this, it's usually best to use a code-chunk and `knitr`. There are a number of graphical paramerters you can set (or ignore)
`out.width` will scale your image accordingly - irrespective of unit/display
`fig.align` should be "left", "right", or "center"
`fig.cap` allows you to provide "mouse over" captions for the image. 
`echo=FALSE` is important if you ONLY want the image (IE the result of the code). If you want the code itself to show, (IE, or echo) set `echo=TRUE`.


```{r, out.width= '50%', fig.align='center', fig.cap='the ipums logo', echo = FALSE}
knitr::include_graphics("imgs/ipums_i_logo.jpg")
```
## Tips

***Autonumber sections** Note the `{-}` used to indicate "do not number this section" eg: preface.

**LABEL EVERYTHING** you'll likely want to reference it later
  * code chunks that produce figures can be referenced via `@\ref(fig:[LABEL])`
  

You can label chapter and section titles using `{#label}` after them, e.g., we can reference Chapter \@ref(unit1). If you do not manually label them, there will be automatic labels anyway,
  * No idea how the automatic references work, so always be sure to declare them.
  * **NOTE** these display as the relevant Chapter `numeral`.
  
  


## Syntax

_italics_ or 
*italics* (can handle spaces)
**bold**
`code`
$equations$

### Math

Randal Pruim features an extensive list of common math expression on their [github page](https://rpruim.github.io/s341/S19/from-class/MathinRmd.html). Here are some quick notes: 

In-line equations can be written within `$` and will be displayed right there: $a^2 + b^2 = c^2$. In contrast, you can also add equation chunks by using `$$` 

This can be coded in-line,  $$\sum_{n=1}^{10} n^2$$, but will result in a page break.


Alternatively, a more "classic" equation chunk:

$$
Plain text doesnt get spaces

how

very



odd

$$


#### more math example


$p$ is unknown but expected to be around 1/3. Standard error will be approximated

$$
SE = \sqrt(\frac{p(1-p)}{n}) \approx \sqrt{\frac{1/3 (1 - 1/3)} {300}} = 0.027
$$

You can also use math in footnotes like this^[where we mention $p = \frac{a}{b}$]. Footnotes are helpful because they re-link to where you left off.

We will approximate standard error to 0.027[^longnote]

[^longnote]: $p$ is unknown but expected to be around 1/3. Standard error will be approximated

    $$
    SE = \sqrt(\frac{p(1-p)}{n}) \approx \sqrt{\frac{1/3 (1 - 1/3)} {300}} = 0.027
    $$

The `longnote` footnote seems particularly useful.


To compile this example to PDF, you need XeLaTeX. You are recommended to install TinyTeX (which includes XeLaTeX): <https://yihui.name/tinytex/>.

<!--chapter:end:40-examples.Rmd-->


# What *is* data?


## POV:  


> In a social science class, your teacher tells you that the CDC
> reports average male height in the United States to be 69
> inches or 5ft 9in. While browsing dating apps, you notice that
> nearly all the men report that they are 6ft or over. You wonder 
> if this is a bias in reporting, or if the area where you live 
> and attend college has significantly taller men. To test your 
> theory, you want to collect data on height from individuals in 
> your data science class to test if males are truly taller on campus 
> than the country average. 
> 
> **Source:** https://www.cdc.gov/nchs/fastats/body-measurements.htm**


### ACTIVITY - collect data on height

**If in-person**
  * Create a histogram (x axis) on a blackboard/wall, have students place their heights 
  with a post-it note
    + Start with one distribution for whole class
    + Repeat with separate distributions for M/F 
    + _DEE: I want to find a way to acknowledge this dichotomy ignores non-binary individuals, and include some suggestions on how to discuss it._


### ACTIVITY - collect data on birth month

  * Create a histogram (x axis) on a blackboard/wall, have students place their heights 
  with a post-it note
    + Students place post-it notes on month of birth
    
**IDEALLY:** Have both a histogram of height AND histogram of birth month visible
at the same time. Compare/contrast distributions of the two data sets.





## Explore


Questions to consider:
  * *Why do we plot data*
  * *What does the* **distribution** *of post-it notes look like?* 
  * *What can you infer from the distribution(s)*
  * *How does the distribution of height differ from birth month?*





### Example Datasets

If you're working through this course on your own, or are unable to facilitate a classroom activity, see the companion R package, `ipumsED`, which includes example data sets for each lesson, as well as custom code and functions to facilitate learning data science with IPUMS data.

_DEE: This could probably be stated at the begining of unit 1_



### ACTIVITY - Calculate by hand


In our hypothetical setup, we are interested in the **average** height.
  * *What does it mean to be* ***average*** 
  * *Which height would you say is average? (eyeballing)*
  * *Which Birth Month would you say is average? - is there one?*
  
  
Write out steps for calculating **mean** - trivial as it may seem.



## Explain

In the context of this example, we **collected** data on height and birth month for individuals in our class. 
Plotting our data allows us to **visualize** the data, making it easy to interpret.


## Ellaborate

### So what is data?

Data is defined as “facts and statistics collected together for 
reference or analysis.”^[This is from the internet and needs to 
be our words] As seen in Figure 1.1, there are two types of data:
quantitative and qualitative. **Quantitative data** are able to 
be expressed in numerical format and are countable. These data 
are either discrete or continuous where **discrete data** uses 
numeric bins. For example, we use our age as discrete 
quantitative data, we round our age to the previous year (eg., 
20, 21, 22). **Continuous data** does not use bins, but rather 
includes all of the fractions between two whole numbers. An 
example could be most physical measures like height, weight, the 
speed at which an individual runs. 



**Qualitative data** describes characteristics or categories and 
can be broken down into two categories, nominal or ordinal. 
**Nominal data** has no inherent ordering but it can be 
categorized. Examples include country or origin, gender, hair 
color, race, etc. **Ordinal data** can both be categorized and 
ordered (e.g., first, second, and third place is a race).


> Going back to our hypothesis of male height on campus, heights 
> are continuous, qualitative data. It is difficult for people to
> report their specific height and you assume that most 
> individuals will report it rounded to the closest inch. This 
> makes the data you will actually use, discrete quantitative 
> data.



### Collecting Data


The first step to answering a research question is to collect 
your data. Broadly, data comes in two forms, primary and 
secondary. (Fig 1.2) **Primary data** is data that is collected 
directly by the researcher. Surveys, observations, 
experimentation, questionnaires, and interviews are all examples 
of primary data. **Secondary data** is collected from published 
or unpublished literature. It is collected by different 
researchers and compiled for use by a second scientist. This type 
of data includes data found in published articles, books, 
journals, biographies, and government records like the US Census.

Once compiled, you now have a data set which is comprised of 
observations and variables. An **observation** is all of the
measures taken for one person or item. A **variable** is what is 
being measured. 

> The US CDC data is secondary, but you are collecting height 
> data yourself in class as a comparison. The survey or 
> questionnaire you use on your classmates is primary data. Each 
> individual is an observation and the variable of interest is 
> height.


### POPULATIONS AND SAMPLING
 
 
**Random Sampling:** It is a sampling method in which all the 
items have an equal chance of being selected and the individuals 
who are selected are just like the ones who are not selected

**Stratified Random Sampling:** It is a process to gather data by
separating the actual population into the distinct subset or 
strata, and then choosing simple random samples from each stratum
Your research question is about the height of all males at your 
college, but recording height data for each individual would be 
very difficult and time consuming. You instead decide to use a 
sample of males in your data science class. This is a random 
sample as each male individual has an equally likely chance of 
being samples (that is, unless a prerequisite exists).

Sampling strategy can lead to **bias**

> If you had chosen a different sample, like the men’s basketball
> team, your results would have been biased. 


## Evaluation


### Review Questions
  * What is one example of collecting/visualizing data from your own life
  * Is height a **continuous** or **discrete** variable and Why?


### Exercises

Brainstorm 3 topics/questions that are of interest to you personally, or academically.
  * What **variable(s)** will you need to collect to study this phenomenon?
  * Describe these variable(s), are they qualitative or quantitative? Continuous or ordinal?


## Glossary {-}


<!--chapter:end:41-Unit1_what_is_data.Rmd-->


# Intro to R, data types, data structures 


In the previous lesson, we began thinking about **data**, how to talk about it, 
and how to **visualize** it. We also talked about one type of average, the **mean.**

## Engage

*What do you think the following code does?*


```{r, eval = FALSE, echo = TRUE}

my_data <- read_excel("//filepath/directory/filename.xlsx")

```



*Hint:* There are 4 "things" in the above code: 
  `my_data`, 
  `<-`, 
  `read_excel()`, 
  `//filepath/directory/filename.xlsx`



### R vs RStudio

* If `R` is the engine, then `RStudio` is the car.

* If `R` is the text, Rstudio is the text-editor.

* `R` is the **programming language**, `Rstudio` is the **Integrated Development Environment (IDE)**.
  * *What do you think some differences are between R and RStudio?*


`RStudio` is a program/app just like Google Chrome or Microsoft Word. Each
of these programs provide a **Graphical User Interface (GUI)**, a pretty way for 
a user to use a mouse and keyboard to do *something.*

An **IDE** is a special kind of program/app that provides MANY tools for writing and running code.



### Orientation to RStudio

_SUGGESTION: Instructor live demos interacting with RStudio while students follow along on computers_



When you first open RStudio, you'll see 3 **panes**, all of which will look fairly empty at the moment.

If you're using the default layout, you should see: 
  * On the left, the `Console` **pane**
  * On the Top-right, the `Environment` **pane**
  * On the Bottom-right, the `Files` **pane**


A keen eye will also notice that each of these **panes** contains multiple **tabs**. We will go over the uses of many of these **tabs**, but for now let's start with `Console.`



The `Console` is where you input `R code`, run it, and see the results.


At it's simplest `RStudio` is a calculator. Try typing `4 + 4` into the `Console`,
then press the `[Enter]` key to run the code. Immediately, `R` prints the result as we see here:

```{r}
4 + 4
```



The `Console` serves as a running log of all your operations for your current session, but it can be helpful to temporarily save your results as `R objects`. We do this using the  **assignment operator** we saw earlier: `<-`


```{r}
answer <- 4+4
```


By default, `Console` only prints results if they have no where else to go. If they're stored as an `Robject`, no result is printed, but you should now see `answer` listed in the `Environment` **pane**.





## Explore - Interacting with R objects  



### Load the Data 



```{r}

dir_path <- file.path("inst","unit1_data")
survey_path <- file.path(dir_path, "data_template.xlsx")

data <- readxl::read_excel(survey_path)
```






What is `data`?? Below we call the `class()` function on `data` and see that it has 3 classes: 
  `tbl_df` , `tbl` , `data.frame`
  
The first two classes, `tbl_df, tbl` indicate it is a special kind of table, in the `tibble` format. In general, you can interact with these like a `matrix` or `data.frame` but they have additional features. 


```{r}
class(data)
```



We can call `colnames()` on data, like a regular `data.frame` or `matrix`. Or we can take advantage of the `tibble` structure and use the `glimpse()` function which provides a succinct summary of your data.

```{r}

colnames(data)

tibble::glimpse(data)

```





### Inspect the Data 

What is `data`?? Below we call the `class()` function on `data` and see that it has 3 classes: 
  `tbl_df` , `tbl` , `data.frame`
  
The first two classes, `tbl_df, tbl` indicate it is a special kind of table, in the `tibble` format. In general, you can interact with these like a `matrix` or `data.frame` but they have additional features. 


```{r}
class(data)
```



We can call `colnames()` on data, like a regular `data.frame` or `matrix`. Or we can take advantage of the `tibble` structure and use the `glimpse()` function which provides a succinct summary of your data.

```{r}

colnames(data)

tibble::glimpse(data)

```





## Summarize Data {-}

### Continuous Data {-}

For continuous data, we often want to summarize our data by describing the **mean, median, and/or range**. **Mean** and **median** describe the *central tendency* of the data, while **range** describes the full extant of the data, as seen below. 

NOTE: if `NA` are present in the data, be sure to use the `na.rm=TRUE` flag for these operations.


```{r}

mean(data$Height_inches, na.rm = T)
median(data$Height_inches, na.rm = T)
range(data$Height_inches, na.rm = T)

```




### All in summary() {-}


**Mean**, **median**, and **range** will all be reported by calling `summary()` on a `numeric vector`, such as `Height_inches`. In addition, the lower and upper quartiles will be reported, along with the number of `NA` responses.


NOTE: `summary()` does NOT require special handling for `NA` values, in fact - it expects them!

```{r}
summary(data$Height_inches)
```


## Mode 

You're probably familiar with **mean** and **median** being talked about with a 
third term, **mode**. The **mode** is the most commonly occuring value in a 
dataset. It's often important to know the **modal response** of survey data. 
While a commonly reported metric(??), there is no `mode()` function included in 
`base r`...  




so we'll just have to create our own!   



### Mode Code 

One common measure of data reported is the mode, or most frequently occuring value. For whatever reason, this is not a default function in R, but we can easily write our own function like so:

```{r}

my_mode <- function(x){
  tt <- table(x) ## find frequencies
  tt <- tt[order(tt, decreasing = TRUE)] ## resort based on freq
  
  ## check number of modes
  max <- max(tt)
  n_max <- sum(tt==max)
  
  
  if(n_max > 1 ){
    warning("More than one mode detected")
    return(tt[tt==max])
  } else {
    ## return only the first value
    return(tt[1]) ## return whatever the highrst frequency is
  }

  
}

```

### Mode Results  


Now that we've created out own `function`, it's easy to find the **mode**


```{r}

my_mode(data$Height_inches)

```



## Visualizing Data 

The above summaries describe data with numbers, but we can also describe data visually.



### Continuous Data - Boxplots 


Univariate continuous data, like height, can be visualized using a box and whisker plot, which shows many of the components of summary:

  * the **median** is the black bar in the middle
  * the **quartiles** (25th and 75th percentiles) are represented by the extents of the boxes
  * The **range** is shown by the whiskers, with outliers shown indvidually, if needed.
  
  
  
```{r}
boxplot(data$Height_inches)
```


### Continuous Data - Histograms 

Continuous data, can also be broken into **bins** and plotted as a **histogram**. The `hist()` function will attempt to find the optimum number of bins for you, but you can specify a different number with the `breaks` argument.


```{r}
hist(data$Height_inches, main = "Histogram of Height", xlab = "Height (in)")
```




### Categorical Data 

Categorical data is already in discrete units. In general with categorical data, we want to count the **frequency** of unique values. There are many ways to do this, but one of the easiest is the `table()` function. Saving the results of the table to an object, `birth_freq`, allows you to save and print the results at any time.

```{r}

birth_freq <- table(data$Birth_Month)

birth_freq
```


We can also visualize our tabulated results using a **barplot** as below.


```{r}

barplot(birth_freq)

```



## Glossary {-}



<!--chapter:end:42-Unit1_data_in_r.Rmd-->



# Comparing Data


**NEEDS A LOT OF WORK**


## Data Distributions

### Normal Distributions

First we'll generate a normal distribution with the `rnorm()` function. This takes 3 arguments: `n, mean, sd`, which you can see filled in below. While we could print out a list of all these values, it's not easy to *understand* a list of numbers
```{r}

normal_dist <- rnorm(n = 100, ## 100 samples
                     mean = 10, ## with a mean of 10
                     sd = 1 ## and a standard deviation of 1
                     )


normal_dist
```


Another better way to look at data would be to **visualize** or **plot** it. One way to to that is with a **histogram**, which groups **continuous values** into **bins**, then plots the **frequency** for each bin. 

In R, we use the `hist()` function to plot a histogram of data. We can (try to) control the number of bins with the `breaks` argument, but note that it doesn't always match up. The `hist()` function will adjust based on the distribution of the data.

```{r norm_dist_plot}
hist(normal_dist,breaks = 5)
```

Another way to visualize this would be with a d

### What *is* normal?
#### Quantitative summaries

5num summary
* Min, 25th percentile, median, 75th percentile, Max

```{r}
tab_normal_dist <- summary(normal_dist)

```


We can print the table in R by calling its name.

```{r}
tab_normal_dist
```

Mean, standard deviation

#### Meaningful Comparisons

How to compare apples to oranges? Standardize the units / standardize the data


```{r}
data1 <- rnorm(n=1000, 
              mean = 100,
              sd = 10)

data2 <- rnorm(n=1000,
               mean = 60, 
               sd = 25)

```
Are these the same distribution?

Any issues??

```{r, rnorm_hist_comp}
layout(matrix(1:2, ncol = 2))
hist(data1)
hist(data2)
```

```{r}
total_range <- range(data1, data2)
```


Are they the same?

```{r, rnorm_hist_comp_true}
layout(matrix(1:2, ncol = 2))
hist(data1, xlim = total_range)
hist(data2, xlim = total_range)
```

Numerically / tabularly

Often times its important to tables of **summary statistics**

```{r}

norm_comp_tab <- rbind(summary(data1),
                       summary(data2))

norm_comp_tab

```


Making the table a little nicer. Also an example of **conditional programming**.


```{r}


rownames(norm_comp_tab) ## they're null


if(is.null(rownames(norm_comp_tab))){
  rownames(norm_comp_tab) <- c("data1", "data2")
}

```
When working with **Rmarkdown** we can take advantage of `knitr` and `pandoc` to nice looking tables even easier.


```{r norm_summary_tab}

knitr::kable(norm_comp_tab)
```
**How** transform the data

Simple transformation (multiply all values by 100)
  * to convert units
  * other examples?

Complex transformations
  * log-transformation (*DEE: not a fan*)
  * z-scores (*DEE: a better option*)

**Why** transform the data?
  * Real world applications?
  * Is it always appropriate to transform data?

  
### Skews

What to do if the data are **not** normal?

##  Statisitcal testing of simple data sets

### t-tests, ANOVA, chi2

## Relationships between variables in simple data sets
### Correlation, Linear Regression
#### Simple LM
#### Complex LM

### Genearlized Linear Model


For now, I have 3 main chapters for each of the main sections: 
  * Basics of data science / R \@ref(unit1) 
  * Applications/critiques using IPUMS data \@ref(unit2)
  * Student-driven projects \@ref(unit3)

Each of these **Chapters** contains multiple sections. We'll likely want to break these sections out into their own `.Rmd` files as they get fleshed out. For now, I'll try to keep the abundance of files limited.

**NOTE:** As these actually get filled out, we will probably want to insert different `part`s to the book (EG, the content of Unit 1 is covered in `Part I`).
  * Declare parts with `# (PART) Part I {-}` immediately before the first chapter `#` it contains.
  
**Topics to include:**
  * What is data?
  * Everything can be data
  * How do we interpret data
  * Tables
  * Plots
  * Univariate distributions
  * What can they tell us
  * Multi-modality in distributions
  * Categorical vs continuous data
  * Don’t need to get ahead of this yet
  * Add in a grouping category - multi state/multi-national dataset
  * Ttest / anova

**Type of Data:**
Age distributions
Specifically generate a dataset with old/young folks over-represented to highlight a bimodal distribution

Start with single state/country
Add a second state/country to demo ttest
Add more to demo anova

Alternatively, income by education level - may be more interesting/relevant to college students (or depressing)

## Intro to R/RStudio
## Reading Data / Distributions
### What *is* a **normal distribution**
#### How normal is it?
show increasingly unclear examples of normal vs not

introduce tests of normality

#### Measuring normality - single sample

reinforce [concept of statistical] **normality**

is a value from a sample? - one way ttest
something about tails

#### comparing normality - two saples

standard / two-way t test

#### comparing more than two - ANOVA

## Glossary {-}
Data
Quantitative
Qualitative
Discrete 
Continuous
Nominal
Ordinal


<!--chapter:end:43-Unit1_comparing_data.Rmd-->

`r if (knitr:::is_html_output()) '
# References {-}
'`

<!--chapter:end:50-references.Rmd-->

# Intro to R/Rstudio

**NOTE:** Some of this can get summarized in the preface, but I think we should flesh out a whole lesson on it anyway. Supplemental/optional

## Engage

### Class poll: Who has used R before

tabulate Y/N 
calculate frequncies
draw boxplot
  side by side counts
  stacked counts
  stacked frequencies
  *which is the most informative*

### Class poll: who has done ANY kind of coding

Might be redundant to do again

### Interpreting R

Describe what the following R code does, in plain words.

```{r, eval = FALSE, echo = TRUE}

my_data <- read_excel("//filepath/directory/filename.xlsx")

```


*Hint:* There are 4 elements in the above code: 
  `my_data`, 
  `<-`, 
  `read_excel()`, 
  `//filepath/directory/filename.xlsx`



## Explore

**see old_ file for more examples**

The best way to learn R is to jump into it. Ideally, this lesson can be in a 
computer lab or with laptop access.


## Explain - lecture/read

## Some Basics

### R vs RStudio vs POSIT

* If `R` is the engine, then `RStudio` is the car.

* If `R` is the text, Rstudio is the text-editor.

* `R` is the **programming language**, `Rstudio` is the **Integrated Development Environment (IDE)**.
  * *What do you think some differences are between R and RStudio?*


`RStudio` is a program/app just like Google Chrome or Microsoft Word. Each
of these programs provide a **Graphical User Interface (GUI)**, a pretty way for 
a user to use a mouse and keyboard to do *something.*

An **IDE** is a special kind of program/app that provides MANY tools for writing and running code.

### We will use Rstudio, to write R scripts that will analyze and visualize data.

R is a language that can DO a lot.
RStudio is a program that lets YOU do a lot USING the R language (and others)

### How to install

Go to posit


## Navigating RStudio

### Lots of Panes, not a pain!

You get used to it, promise.

## Writing R

### Writing R in Console

Direct input
Direct output

LIMITED

### Writing R in Scripts (The 4th pane)

Save your work, find typos, re-run easy!


## Object-oriented progrmaing in R

### R objects
### R classes
### R Projects

## How to find help

### In R
### the web!

stackexchange
rdrr.io


## Elaborate


Set up R project
Read data into R
For one variable (height)
Visualize data
Calculate mean, median, range
HOW TO FIND MODE??

save out results file



## Evaluate/Exercises

Students should set up their own Rproject and repeat the steps above for EACH 
variable, including height. 
Students should replicate in-class work by reading in class data and finding mean, median, and mode for ALL variables (there may be errors!)
Reflect: What `class()` is each variable. Does it make sense to ALWAYS look for mean, median, mode? 

 

<!--chapter:end:60-intro_to_R.Rmd-->

